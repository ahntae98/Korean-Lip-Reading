{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def clustering(str,list1,list2):\n",
    "    score = []\n",
    "    for i in range(len(list1)):\n",
    "        ratio = SequenceMatcher(None,str,list1[i]).ratio()\n",
    "        score.append(ratio)\n",
    "    index = score.index(max(score))\n",
    "    return list2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹캠을 열었습니다. 'q' 키를 눌러 종료하세요.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def check_webcam():\n",
    "    # 웹캠 초기화\n",
    "    cap = cv2.VideoCapture(2)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"웹캠을 열 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"웹캠을 열었습니다. 'q' 키를 눌러 종료하세요.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"프레임을 읽을 수 없습니다.\")\n",
    "                break\n",
    "\n",
    "            # 프레임을 화면에 표시\n",
    "            cv2.imshow('Webcam', frame)\n",
    "\n",
    "            # 'q' 키를 누르면 종료\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_webcam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Evaluating...\n",
      "Predicted Syllable: 임\n",
      "Predicted Word: 임\n",
      "Evaluating...\n",
      "Predicted Syllable: 임\n",
      "Predicted Word: 임임\n",
      "Evaluating...\n",
      "Predicted Syllable: 안\n",
      "Predicted Word: 임임안\n",
      "Evaluating...\n",
      "Predicted Syllable: 안\n",
      "Predicted Word: 임임안안\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import dlib\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# CUDA 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# dlib의 얼굴 랜드마크 모델 경로 설정\n",
    "landmark_model_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(landmark_model_path)\n",
    "\n",
    "# 레이블 매핑\n",
    "label_to_idx = {0: '아',\n",
    " 1: '왠',\n",
    " 2: '어',\n",
    " 3: '우',\n",
    " 4: '애',\n",
    " 5: '안',\n",
    " 6: '마',\n",
    " 7: '으',\n",
    " 8: '여',\n",
    " 9: '임',\n",
    " 10: '오',\n",
    " 11: '워'}\n",
    "idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "\n",
    "# MobileNetV2 + LSTM + Dropout 모델 정의\n",
    "class MobileNetV2LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.6):\n",
    "        super(MobileNetV2LSTM, self).__init__()\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        mobilenet.features = nn.Sequential(\n",
    "            *list(mobilenet.features),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # 추가: AdaptiveAvgPool2d로 마지막 출력 크기를 (1, 1)로 만듦\n",
    "        )\n",
    "        self.mobilenet = mobilenet.features\n",
    "        self.lstm = nn.LSTM(1280, 256, batch_first=True)  # MobileNetV2의 출력 크기 1280\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout 레이어 추가\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.size()\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        x = self.mobilenet(x).squeeze(-1).squeeze(-1)  # (batch_size * seq_length, 1280, 1, 1) -> (batch_size * seq_length, 1280)\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)  # Dropout 적용\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "# 모델 초기화 및 로드\n",
    "num_classes = len(label_to_idx)\n",
    "model = MobileNetV2LSTM(num_classes).to(device)\n",
    "model.load_state_dict(torch.load('lip_reading_model_1.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "def extract_lip_shape(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        lip_points = []\n",
    "\n",
    "        # 입술 영역의 랜드마크 점들을 추출\n",
    "        for i in range(48, 61):\n",
    "            x = landmarks.part(i).x\n",
    "            y = landmarks.part(i).y\n",
    "            lip_points.append((x, y))\n",
    "\n",
    "        return lip_points\n",
    "    return None\n",
    "\n",
    "def crop_lips_from_frame(frame, lip_points):\n",
    "    if lip_points is None:\n",
    "        return None\n",
    "\n",
    "    # 입술 영역의 좌표들로부터 사각형 영역 계산\n",
    "    x_coords = [p[0] for p in lip_points]\n",
    "    y_coords = [p[1] for p in lip_points]\n",
    "\n",
    "    x_min = min(x_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    y_max = max(y_coords)\n",
    "\n",
    "    # 입술 영역을 사각형으로 자르기\n",
    "    cropped_lips = frame[y_min:y_max, x_min:x_max]\n",
    "    return cropped_lips\n",
    "\n",
    "def predict_lip_sequence(lip_frames):\n",
    "    images = [transform(Image.fromarray(frame)) for frame in lip_frames]\n",
    "    images_tensor = torch.stack(images).unsqueeze(0).to(device)  # (1, sequence_length, C, H, W)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return label_to_idx[predicted.item()]\n",
    "\n",
    "def save_frames_to_folder(frames, label, folder_path='saved_frames'):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    for i, frame in enumerate(frames):\n",
    "        cv2.imwrite(os.path.join(folder_path, f\"{label}_{i}.png\"), frame)\n",
    "\n",
    "def clustering(str, list1, list2):\n",
    "    score = []\n",
    "    for i in range(len(list1)):\n",
    "        ratio = SequenceMatcher(None, str, list1[i]).ratio()\n",
    "        score.append(ratio)\n",
    "    index = score.index(max(score))\n",
    "    if len(list2[index]) == len(str):\n",
    "        return list2[index]\n",
    "    return None\n",
    "\n",
    "def put_korean_text(image, text, position, font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf', font_size=32, color=(255, 255, 255)):\n",
    "    \"\"\"한글 텍스트를 이미지에 합성하는 함수\"\"\"\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "    return cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def main():\n",
    "    # 웹캠 초기화\n",
    "    cap = cv2.VideoCapture(2)\n",
    "\n",
    "    frame_count = 0\n",
    "    save_frame_count = 0\n",
    "    lip_frames = []\n",
    "    recording = False\n",
    "    predictions = []\n",
    "    predicted_word = \"\"\n",
    "    clustered_word = \"\"\n",
    "    \n",
    "    str_list = ['안여',\n",
    "                '아마오',\n",
    "                '임으어',\n",
    "                '오아워',\n",
    "                '왠안아',\n",
    "                '우우',\n",
    "                '오아',\n",
    "                '오아워',\n",
    "                '오여',\n",
    "                '아아애']\n",
    "    answer_list = ['안녕',\n",
    "                    '아마도',\n",
    "                    '힘들어',\n",
    "                    '놀라워',\n",
    "                    '괜찮아',\n",
    "                    '누구',\n",
    "                    '좋아',\n",
    "                    '고마워',\n",
    "                    '졸려',\n",
    "                    '사랑해']\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            lip_points = extract_lip_shape(frame)\n",
    "\n",
    "            if lip_points:\n",
    "                cropped_lips = crop_lips_from_frame(frame, lip_points)\n",
    "                if cropped_lips is not None and cropped_lips.size > 0:\n",
    "                    cv2.imshow('Cropped Lips', cropped_lips)\n",
    "                    if recording and frame_count % 2 == 0:  # 3프레임마다 하나씩 저장\n",
    "                        lip_frames.append(cropped_lips)\n",
    "                        save_frame_count += 1\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            if save_frame_count == 10:  # 10장의 이미지를 저장\n",
    "                print(\"Evaluating...\")\n",
    "                result = predict_lip_sequence(lip_frames)\n",
    "                predictions.append(result)\n",
    "                print(f'Predicted Syllable: {result}')\n",
    "                save_frames_to_folder(lip_frames, f'{result}_segment_{len(predictions)}')\n",
    "                lip_frames = []\n",
    "                save_frame_count = 0\n",
    "                recording = False\n",
    "\n",
    "                # 저장된 예측값을 바탕으로 단어 유추\n",
    "                predicted_word = ''.join(predictions)\n",
    "                print(f'Predicted Word: {predicted_word}')\n",
    "\n",
    "                clustered_word = clustering(predicted_word, str_list, answer_list)\n",
    "                if clustered_word:\n",
    "                    print(clustered_word)\n",
    "\n",
    "            # 화면에 예측 단어 출력\n",
    "            if clustered_word:\n",
    "                frame = put_korean_text(frame, f'Predicted Word: {clustered_word}', (50, 100))\n",
    "\n",
    "            cv2.imshow('Webcam', frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break          \n",
    "            elif key == ord('s'):\n",
    "                recording = True\n",
    "            elif key == ord('e'):  # 'e' 입력시 예측 배열 초기화\n",
    "                predictions = []\n",
    "                predicted_word = \"\"\n",
    "                clustered_word = \"\"\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/delivery/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/delivery/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "mobilenet = MobileNetV2LSTM(num_classes=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
